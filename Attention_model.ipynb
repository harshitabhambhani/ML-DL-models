{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwJlmsYNsX7T0Jv4/5Dolq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitabhambhani/ML-DL-models/blob/main/Attention_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4jXmqxcqIVL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention model is a mechanism used in machine learning and deep learning to improve the performance of tasks that involve sequential data, such as natural language processing and computer vision. The attention mechanism allows the model to focus on specific parts of the input sequence when making predictions, rather than considering the entire input sequence at once. This helps the model capture long-range dependencies and improves its ability to handle variable-length input.\n",
        "\n",
        "The attention mechanism works by assigning different weights to different parts of the input sequence, indicating the importance of each part for the current prediction. These weights are dynamically adjusted during the training process based on the context and the current state of the model."
      ],
      "metadata": {
        "id": "UQrswALeqKvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input sequence length and vocabulary size\n",
        "sequence_length = 10\n",
        "vocab_size = 1000\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=(sequence_length,))\n",
        "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=64)(encoder_inputs)\n",
        "encoder_lstm = LSTM(128, return_sequences=True)(encoder_embedding)\n",
        "\n",
        "# Define the decoder\n",
        "decoder_inputs = Input(shape=(sequence_length,))\n",
        "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=64)(decoder_inputs)\n",
        "decoder_lstm = LSTM(128, return_sequences=True)(decoder_embedding)\n",
        "\n",
        "# Apply attention layer\n",
        "attention = Attention()([encoder_lstm, decoder_lstm])\n",
        "\n",
        "# Concatenate attention output with decoder LSTM output\n",
        "decoder_combined_context = tf.concat([decoder_lstm, attention], axis=-1)\n",
        "\n",
        "# Apply a dense layer to produce the output sequence\n",
        "decoder_outputs = Dense(vocab_size, activation='softmax')(decoder_combined_context)\n",
        "\n",
        "# Create the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVSrk1tDqk7O",
        "outputId": "5fb97642-e59b-4430-da57-c0b36bba182e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)        [(None, 10)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None, 10)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, 10, 64)               64000     ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, 10, 64)               64000     ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               (None, 10, 128)              98816     ['embedding_5[0][0]']         \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)               (None, 10, 128)              98816     ['embedding_4[0][0]']         \n",
            "                                                                                                  \n",
            " attention_1 (Attention)     (None, 10, 128)              0         ['lstm_4[0][0]',              \n",
            "                                                                     'lstm_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)      (None, 10, 256)              0         ['lstm_5[0][0]',              \n",
            "                                                                     'attention_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 10, 1000)             257000    ['tf.concat[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 582632 (2.22 MB)\n",
            "Trainable params: 582632 (2.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}